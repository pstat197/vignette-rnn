---
title: "RNNs for Movie Review Sentiment Analysis"
author: " Quinlan Wilson, Akhil Gorla,Ella Yang, Lorreta Lu, Anish Senthil"
format: html
execute:
  echo: true
  warning: false
  message: false
---

## Introduction

In this vignette, we develop a Recurrent Neural Network (RNN) using a Long Short-Term Memory (LSTM) architecture to classify IMDB movie reviews as either positive or negative. Sentiment analysis is one of the most common applications of natural language processing (NLP) and provides an excellent setting for understanding how sequential models learn patterns, context, and emotional tone in text.

Traditional feedforward neural networks treat every input as independent, making them poorly suited to sequential data where the order of information matters. RNNs solve this by incorporating a hidden state that evolves as the model processes each word, giving the network a form of “memory.” LSTMs extend this ability through specialized gating mechanisms that allow the model to selectively retain, overwrite, or forget information over long sequences—helping them overcome the vanishing gradient problem.

In this vignette, we walk through the entire modeling pipeline:

- loading and exploring the dataset

- tokenizing text and preparing input sequences

- building a model using an Embedding layer + Bidirectional LSTM

- training and evaluating the network

- analyzing learned word embeddings

- generating predictions on new text

- discussing model limitations

#### What is a RNN

A Recurrent Neural Network processes input one element at a time while maintaining a hidden "memory" state:

$$
h_t = \tanh \left( W_x x_t + W_h h_{t-1} + b \right)
$$

Where:

- \(x_t\) is the input at time step \(t\)
- \(h_t\) is the hidden state at time \(t\)
- \(h_{t-1}\) is the previous hidden state
- \(W_x, W_h\) are learned weight matrices
- \(b\) is a bias vector

This recurrent computation allows the model to “remember” earlier words while processing later ones.

However, simple RNNs cannot remember information reliably across long sequences due to vanishing/ exploding gradients. LSTMs address this by using a cell state and three gates:

- input gate — decides what new information to write

- forget gate — decides what to erase

- output gate — decides what information becomes visible

Because sentiment often depends on long-range dependencies (“I thought the movie would be great, but it was awful”), LSTMs are far more effective than basic RNNs.

## Dataset and Preprocessing

Before an RNN can learn from text, the text must be converted to numbers.  
We encode sentiment labels (positive = 1, negative = 0), tokenize text, restrict vocabulary to 10,000 words, and pad or truncate sequences to a fixed length.

```{r, warning=FALSE, message=FALSE}

# This vignette requires TensorFlow and Keras.
# If TensorFlow is not installed, run in the R console:
#   install.packages("tensorflow")
#   library(tensorflow)
#   install_tensorflow()
# This model is based on Keras
# If the Keras not installed or automatically installed Keras3
# Please change to Keras(Old version)
#   install.packages("keras)
#   library(keras)
#   install_keras()

library(keras) # for deep learning
library(tidyverse) # for read_csv and data manipulation
library(caret) # for train/test split
library(tensorflow)

set.seed(3434)
# Read in the data
movie_reviews <- read_csv("data/IMDB Dataset.csv")

# Select the column with info on the emotion of review
sentiment <- movie_reviews$sentiment

# Check class balance (should be about equal in our case exactly equal)
table(sentiment)

# Convert labels to binary (1 = positive, 0 = negative)
movie_reviews$sentiment_binary <- ifelse(movie_reviews$sentiment == "positive", 1, 0)

# Tokenization (converting text into integers)
# Neural networks can't work with raw text so tokenization is used.
# On a basic level tokenization changes each word into an integer based on frequency. 
# We only use the top 10k most frequent words
max_words <- 10000

# RNNs need all sequences to be the same length.
# So by choosing 500 if a review has less than 500 words
# those extras are removed and if it has less than 500
# we add zeros to make it the same length. 
max_len <- 250
review_len <- sapply(strsplit(movie_reviews$review, " "), length)
hist(review_len, breaks = 50, main="Distribution of Review Lengths", xlab = "Words per review", xlim = c(0, 1250))
summary(review_len)

# Create a tokenizer and fit it to our review text
tokenizer <- text_tokenizer(num_words = max_words)
tokenizer %>% fit_text_tokenizer(movie_reviews$review)

word_counts <- tokenizer$word_counts
word_freq <- sort(unlist(word_counts), decreasing = T)
# Top 20 words
head(word_freq, 20)

# How many words appear only once or twice
sum(word_freq == 1)
sum(word_freq == 2)

# Since only 52k words appear once and 15k twice setting
# max words to 10k makes sense as these rare words add
# much to our classification model.
# Convert each review to a sequence of integers
sequences <- texts_to_sequences(tokenizer, movie_reviews$review)

# Make every sequences have the same length
x <- pad_sequences(sequences = sequences, maxlen = max_len)
y <- movie_reviews$sentiment_binary

# split the dataset into training and testing (80/20)
train_index <- createDataPartition(y, p = 0.8, list = F)
x_train <- x[train_index, ]
x_test <- x[-train_index, ]
y_train <- y[train_index]
y_test <- y[-train_index]
```

The dataset contains IMDB movie reviews with text and sentiment labels *(positive/negative)*. We first convert the sentiment labels into binary values, mapping positive $\rightarrow$ 1 and negative $\rightarrow$ 0.

Next, we perform tokenization, which converts each word into an integer index based on its frequency or identity. For example:

- “the" $\rightarrow$ 666757

- "movie" $\rightarrow$ 87050

- "film" $\rightarrow$ 77678

This step transforms variable-length text into sequences of integers that the RNN can process.

Because reviews differ in length, we then pad or truncate each sequence to a fixed maximum length. Reviews longer than the limit are cut, and shorter ones are padded with zeros. This ensures a uniform input shape for the model.

Finally, we restrict the vocabulary to the top 10,000 most frequent words, removing extremely rare words that add noise and do not help with training.

## Model Architecture: Embedding + Bidirectional LSTM

RNNs work on sequences of numbers, but these numbers (word indices) have no meaningful geometric structure. The first layer in our model is therefore an Embedding layer, which transforms each integer token into a dense vector that captures semantic meaning.

Each word is mapped to a 64-dimensional vector that the model learns during training. These vectors are not random—they encode relationships such as:

- “good” and “great” appearing in similar vector directions

- “boring” and “awful” clustering in negative sentiment regions

- functional words (“the”, “and”, “to”) lying near the origin

The embedding layer lets the model understand words in a continuous space rather than treating them as unrelated symbols.

We use 64-dimensional embedding.  
The output for each review is a matrix (sequence length $\times$ 64).

This means:

- one LSTM reads the sequence from left to right

- another LSTM reads the sequence from right to left

- their outputs are concatenated, giving a 128-dimensional representation

This structure captures context from both past and future words.

A final dense layer with a sigmoid activation maps the representation to a probability between 0 and 1, indicating how likely the review is positive.

```{r}
embedding_dim <- 64

model <- keras_model_sequential() %>%
  layer_embedding(
    input_dim  = max_words,
    output_dim = embedding_dim

  ) %>%
  bidirectional(
    layer_lstm(units = 64, dropout = 0.2, recurrent_dropout = 0.2)
  ) %>%
  layer_dense(units = 1, activation = "sigmoid")
summary(model)
```
Model architecture:

1. **Embedding layer** - converts word indices to 64-dimensional vectors.  
2. **Bidirectional LSTM** - outputs a 128-dimensional representation.  
3. **Dense output layer** - sigmoid activation gives probability of positive sentiment.

This allows capturing word meaning, order, and context.

#### Training the Model

We train the model using:

- Adam optimizer  
- Binary cross-entropy loss  

Training involves multiple epochs, with a validation split to monitor overfitting.
```{r, eval=FALSE}
# Compile the model: Adam optimizer + binary cross-entropy for binary labels
model %>% compile(
  optimizer = "adam",
  loss = "binary_crossentropy",
  metrics = "accuracy"
)

# Train the model for 3 epochs.
# validation_split = 0.2 uses 20% of the training data as validation
history <- model %>% fit(
  x_train, y_train,
  epochs = 3,
  batch_size = 30,
  validation_split = 0.2
)

# Plot accuracy and loss over epochs (passes)
plot(history)

```
We compile the model with the Adam optimizer and binary cross-entropy loss, then train for 3 epochs while monitoring validation performance.

Training history shows changes in accuracy and loss. Ideally, both training and validation loss decrease or stabilize.

#### Visual Word Embeddings

One of the most fascinating components of deep NLP models is the embedding matrix. After training, each of the 10,000 words has a learned 64-dimensional representation. To visualize these embeddings:

- Extract the embedding matrix from the trained model

- Apply Principal Component Analysis (PCA) to compress the vectors into 2 dimensions

- Plot a subset of words in the 2D plane

Surprisingly, even this crude dimensionality reduction often reveals clear structure:

- Positive sentiment words form clusters (e.g., “wonderful”, “great”, “brilliant”).

- Negative sentiment words group together in a different region (“worst”, “awful”, “boring”).

- High-frequency neutral words (“the”, “and”, “it”) gather around the center.

This emergent structure is learned directly from the review data—no manual labeling is required. It demonstrates how the embedding layer acts as the model’s internal representation of meaning.

```{r, eval=FALSE}
# Extract the embedding matrix from the first layer
embeddings <- get_weights(model$layers[[1]])[[1]]

# Perform PCA on a subset of embeddings (skip index 1 which is usually padding)
pca_res <- prcomp(embeddings[2:1000, ])

# Take the first 2 principal components
pca_df <- data.frame(
  PC1 = pca_res$x[,1],
  PC2 = pca_res$x[,2],
  word = names(tokenizer$word_index)[1:999]
)

# Scatterplot of words in the 2D PCA space
ggplot(pca_df, aes(x = PC1, y = PC2, label=word)) +
  geom_text(size=3) +
  ggtitle("Word Embeddings PCA Projection")
```


This helps us interpret learned semantic structures.

#### Evaluation

After training, we now evaluate the trained model on the held-out test set to assess how well it generalizes to unseen reviews.

```{r, eval=FALSE}
# Evaluate the model on the held-out test data
model %>% evaluate(x_test, y_test)
```

Metrics returned include accuracy and loss.  
A good model should achieve high accuracy and low loss on unseen data.

#### Predicting Sentiment on New Examples

Once trained, the model can classify any new review by:

- Tokenizing the input text with the same tokenizer

- Padding to the standard sequence length

- Passing the vector through the mode
Predictions > 0.5 $\rightarrow$ positive.

Examples:

- "This is the best movie I have ever seen!" $\rightarrow$ positive  
- "The acting in this movie was horrible." $\rightarrow$ negative  

This shows generalization to new text.
```{r, eval=FALSE}
# Example sentences not seen during training
examples <- c(
  "This is the best movie I have ever seen!",
  "The acting in this movie was horrible."
)

# Tokenize and pad the example sentences in the same way as the training data

examples_seq <- texts_to_sequences(tokenizer, examples)
examples_pad <- pad_sequences(examples_seq, maxlen= max_len)

# Predict sentiment probabilities
predictions <- model %>% predict(examples_pad)

# Convert probabilities to class labels
pred_label <- ifelse(predictions > 0.5, "positive", "negative")
pred_label
```

#### Limitations on Model

While LSTMs were state-of-the-art for many years, they have notable limitations:

1. Difficulty with very long documents

Even with gating, long-range dependencies eventually fade. Sentiment expressed at the end of a long review may outweigh earlier statements, but LSTMs may not fully retain early context.

2. No attention mechanism

All words are treated with roughly equal importance. The model does not learn which specific phrases drive sentiment unless they appear consistently.

3. Word-level tokenization loses nuance

Misspellings (“gud”, “goood”), unseen slang, and sarcasm often trip up LSTMs that rely on fixed vocabulary indexing.

4. Transformers outperform LSTMs

Modern NLP models (BERT, DistilBERT, RoBERTa, GPT) use self-attention to capture global relationships more efficiently and handle longer text with far greater accuracy.

Despite these limitations, LSTMs remain excellent pedagogical models for understanding sequence modeling.
